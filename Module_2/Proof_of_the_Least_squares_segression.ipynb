{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f8579e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b49a0e5",
   "metadata": {},
   "source": [
    "# least square method for linear regrssion model\n",
    "  $$ \\hat{y} = \\theta_0 + \\theta_1 x_i  $$\n",
    "where\n",
    "- $\\theta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}$\n",
    "- $\\theta_0 = \\bar{y} - \\theta_1 \\bar{x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e9395",
   "metadata": {},
   "source": [
    "## Proof of the Least Squares Regression Slope Formula\n",
    "\n",
    "To derive the formula for $\\theta_1$ in **simple linear regression**, we start from the **least squares minimization principle**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b80fc",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Define the Linear Model\n",
    "In simple linear regression, we model \\( y \\) as a linear function of \\( x \\):\n",
    "\n",
    "$$ y_i = \\theta_0 + \\theta_1 x_i + \\epsilon_i $$\n",
    "\n",
    "where:\n",
    "- $ \\theta_0 $ is the **intercept**,\n",
    "- $ \\theta_1 $ is the **slope** (which we aim to find),\n",
    "- $ \\epsilon_i$  represents the **error** term.\n",
    "\n",
    "\n",
    "### Step 2: Define the Cost Function\n",
    "The **least squares method** minimizes the sum of squared errors:\n",
    "\n",
    "$$ J(\\theta_0, \\theta_1) = \\sum (y_i - (\\theta_0 + \\theta_1 x_i))^2 $$\n",
    "\n",
    "### Step 3: Compute Partial Derivatives\n",
    "To minimize $J(\\theta_0, \\theta_1)$, we take **partial derivatives** with respect to $\\theta_0$ and $\\theta_1$, setting them to zero.\n",
    "\n",
    "#### Derivative with respect to $\\theta_0$:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta_0} = -2 \\sum (y_i - (\\theta_0 + \\theta_1 x_i)) = 0 $$\n",
    "\n",
    "Solving for $\\theta_0$:\n",
    "\n",
    "$$ \\theta_0 = \\bar{y} - \\theta_1 \\bar{x} $$\n",
    "\n",
    "#### Derivative with respect to $\\theta_1$:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta_1} = -2 \\sum x_i (y_i - (\\theta_0 + \\theta_1 x_i)) = 0 $$\n",
    "\n",
    "Substituting $\\theta_0 = \\bar{y} - \\theta_1 \\bar{x}$:\n",
    "\n",
    "$$ \\sum x_i (y_i - (\\bar{y} - \\theta_1 \\bar{x} + \\theta_1 x_i)) = 0 $$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$ \\sum x_i y_i - \\sum x_i \\bar{y} + \\theta_1 \\sum x_i \\bar{x} - \\theta_1 \\sum x_i^2 = 0 $$\n",
    "\n",
    "Rearrange:\n",
    "\n",
    "$$ \\theta_1 \\sum (x_i^2 - x_i \\bar{x}) = \\sum (x_i y_i - x_i \\bar{y}) $$\n",
    "Using **summation properties**:\n",
    "\n",
    "$$ \\theta_1 * x_i \\sum (x_i - \\bar{x}) = x_i \\sum ( y_i - \\bar{y}) $$\n",
    "\n",
    "Factor  $ (x_i - \\bar{x}) $\n",
    "$$ \\theta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} $$\n",
    "\n",
    "Thus, we have **derived the formula for the slope** $\\theta_1$ "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
